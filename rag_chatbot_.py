# -*- coding: utf-8 -*-
"""RAG Chatbot .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pdi990FhEnKRgS_Ba4L9k0RvR5NcLUPZ
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install openai chromadb==0.3.29 langchain tiktoken
# %pip install -U langchain-community
# %pip install faiss-cpu
# %pip install sentence-transformers

# Import necessary libraries after resolving conflicts and installing dependencies
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import bs4

from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from transformers import pipeline
from langchain import LLMChain
from langchain.llms import HuggingFacePipeline
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import SimpleSequentialChain

# Import necessary libraries
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain.docstore import InMemoryDocstore
from transformers import pipeline
import numpy as np
import faiss

import os

# List input data files under the input directory
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

file_path = 'Final_IC.csv'
constitution_data = pd.read_csv(file_path)

constitution_sections = constitution_data['article_id'] + ": " + constitution_data['article_desc']

constitution_data

constitution_sections

documents = [Document(page_content=text) for text in constitution_sections]

text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)
chunked_documents = text_splitter.split_documents(documents)

# Use Hugging Face embeddings for indexing
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

index = FAISS.from_documents(chunked_documents, embedding_model)

model_name = "google/flan-t5-large"
hf_pipeline = pipeline("text2text-generation", model=model_name)

llm = HuggingFacePipeline(pipeline=hf_pipeline)

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=index.as_retriever()
)

# response = rag_chain.run({"query": "Protection in respect of conviction for offences"})
# print(response)

embedded_docs = [(doc, embedding_model.embed_query(doc.page_content)) for doc in documents]
doc_texts = [doc.page_content for doc in documents]
embeddings = [embedding_model.embed_query(text) for text in doc_texts]

dimension = len(embeddings[0])
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

docstore = InMemoryDocstore(dict(enumerate(documents)))
vector_store = FAISS(embedding_function=embedding_model, index=index, docstore=docstore, index_to_docstore_id={i: i for i in range(len(documents))})


memory = ConversationBufferMemory(output_key='result')

#  Create a retrieval-based QA chain
retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 2})

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Attach memory buffer to the chain
qa_chain.memory = memory

def query_with_memory(query: str):
    result = qa_chain({"query": query})
    output_for_memory = {"result": result['result']}
    memory.save_context({"query": query}, output_for_memory)
    memory_content = memory.load_memory_variables({})
    print("Memory Buffer Content:", memory_content)
    return result

# query = "Prohibition of employment of children in factories, etc"
# response = query_with_memory(query)
# print("Response:", response['result'])



